# Лабораторная работа №3: Обучение двухслойной ИНС для XOR

## Тема
Реализация алгоритма обратного распространения ошибки (backpropagation) для обучения двухслойной сети функции XOR.

## Описание
Функция XOR (исключающее ИЛИ) нелинейно разделима, поэтому её нельзя реализовать на однослойном персептроне. 
В работе используется двухслойная сеть:
- Входной слой: 2 входа + bias
- Скрытый слой: 2 нейрона с активацией tanh
- Выходной слой: 1 нейрон с сигмоидой

## Структура сети
text
bias = 1
   \
    \
x1 --- h1 (tanh) ---
    /                \
   /                  \
x2 --- h2 (tanh) --- y (сигмоида)
  \                 /
   \               /
bias = 1 ---------/
text

## Математическая основа
- Прямой проход: вычисляем выходы всех нейронов
- Обратный проход: считаем ошибки и распространяем их назад
- Корректировка весов: градиентный спуск

## Функции активации
- **Скрытый слой**: tanh (гиперболический тангенс)
  - диапазон: [-1, 1]
  - производная: 1 - tanh²
  
- **Выходной слой**: сигмоида
  - диапазон: [0, 1] (удобно для бинарной классификации)
  - производная: сигмоида * (1 - сигмоида)

## Файлы
- `xor_backprop.py` - основная реализация алгоритма
- `network_visualization.py` - визуализация результатов

## Запуск

# Обучить сеть
python xor_backprop.py

# Посмотреть визуализацию
python network_visualization.py
Ожидаемый результат
Сеть должна научиться давать правильные ответы:

(0,0) → 0

(0,1) → 1

(1,0) → 1

(1,1) → 0

Из-за нелинейности задачи может потребоваться много эпох (1000+).